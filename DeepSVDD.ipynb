{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHgtppYB/t8F1kwakR+CPq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwP5eeIEr-73","executionInfo":{"status":"ok","timestamp":1676961783717,"user_tz":-540,"elapsed":132901,"user":{"displayName":"오윤주","userId":"02786696648684205101"}},"outputId":"2908a806-0461-4887-fb2b-3b75e90edacc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import easydict \n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score"],"metadata":{"id":"LHXmApXt4Qiu","executionInfo":{"status":"ok","timestamp":1676964626010,"user_tz":-540,"elapsed":5788,"user":{"displayName":"오윤주","userId":"02786696648684205101"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["class MNIST_loader(data.Dataset):\n","    \"\"\"Preprocessing을 포함한 dataloader를 구성\"\"\"\n","    def __init__(self, data, target, transform):\n","        self.data = data\n","        self.target = target\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.target[index]\n","        if self.transform:\n","            x = Image.fromarray(x.numpy(), mode='L')\n","            x = self.transform(x)\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","def get_mnist(args, data_dir='../data/'):\n","    \"\"\"get dataloders\"\"\"\n","    # min, max values for each class after applying GCN (as the original implementation)\n","    min_max = [(-0.8826567065619495, 9.001545489292527),\n","                (-0.6661464580883915, 20.108062262467364),\n","                (-0.7820454743183202, 11.665100841080346),\n","                (-0.7645772083211267, 12.895051191467457),\n","                (-0.7253923114302238, 12.683235701611533),\n","                (-0.7698501867861425, 13.103278415430502),\n","                (-0.778418217980696, 10.457837397569108),\n","                (-0.7129780970522351, 12.057777597673047),\n","                (-0.8280402650205075, 10.581538445782988),\n","                (-0.7369959242164307, 10.697039838804978)]\n","\n","    transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Lambda(lambda x: global_contrast_normalization(x)),\n","                                    transforms.Normalize([min_max[args.normal_class][0]],\n","                                                         [min_max[args.normal_class][1] \\\n","                                                         -min_max[args.normal_class][0]])])\n","    train = datasets.MNIST(root=data_dir, train=True, download=True)\n","    test = datasets.MNIST(root=data_dir, train=False, download=True)\n","\n","    x_train = train.data\n","    y_train = train.targets\n","\n","    x_train = x_train[np.where(y_train==args.normal_class)]\n","    y_train = y_train[np.where(y_train==args.normal_class)]\n","                                    \n","    data_train = MNIST_loader(x_train, y_train, transform)\n","    dataloader_train = DataLoader(data_train, batch_size=args.batch_size, \n","                                  shuffle=True, num_workers=0)\n","    \n","    x_test = test.data\n","    y_test = test.targets\n","    \n","    # Normal class인 경우 0으로 바꾸고, 나머지는 1로 변환 (정상 vs 비정상 class)\n","    y_test = np.where(y_test==args.normal_class, 0, 1)\n","\n","    data_test = MNIST_loader(x_test, y_test, transform)\n","    dataloader_test = DataLoader(data_test, batch_size=args.batch_size, \n","                                  shuffle=False, num_workers=0)\n","    return dataloader_train, dataloader_test\n","    \n","    \n","def global_contrast_normalization(x):\n","    \"\"\"Apply global contrast normalization to tensor. \"\"\"\n","    mean = torch.mean(x)  # mean over all features (pixels) per sample\n","    x -= mean\n","    x_scale = torch.mean(torch.abs(x))\n","    x /= x_scale\n","    return x"],"metadata":{"id":"YbFXe_we4RHb","executionInfo":{"status":"ok","timestamp":1676966352972,"user_tz":-540,"elapsed":524,"user":{"displayName":"오윤주","userId":"02786696648684205101"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["class DeepSVDD_network(nn.Module):\n","    def __init__(self, z_dim=32):\n","        super(DeepSVDD_network, self).__init__()\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n","        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n","        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n","        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n","        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.pool(F.leaky_relu(self.bn1(x)))\n","        x = self.conv2(x)\n","        x = self.pool(F.leaky_relu(self.bn2(x)))\n","        x = x.view(x.size(0), -1)\n","        return self.fc1(x)\n","\n","\n","class pretrain_autoencoder(nn.Module):\n","    def __init__(self, z_dim=32):\n","        super(pretrain_autoencoder, self).__init__()\n","        self.z_dim = z_dim\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n","        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n","        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n","        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n","        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n","\n","        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n","        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n","        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n","        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n","        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n","        \n","    def encoder(self, x):\n","        x = self.conv1(x)\n","        x = self.pool(F.leaky_relu(self.bn1(x)))\n","        x = self.conv2(x)\n","        x = self.pool(F.leaky_relu(self.bn2(x)))\n","        x = x.view(x.size(0), -1)\n","        return self.fc1(x)\n","   \n","    def decoder(self, x):\n","        x = x.view(x.size(0), int(self.z_dim / 16), 4, 4)\n","        x = F.interpolate(F.leaky_relu(x), scale_factor=2)\n","        x = self.deconv1(x)\n","        x = F.interpolate(F.leaky_relu(self.bn3(x)), scale_factor=2)\n","        x = self.deconv2(x)\n","        x = F.interpolate(F.leaky_relu(self.bn4(x)), scale_factor=2)\n","        x = self.deconv3(x)\n","        return torch.sigmoid(x)\n","        \n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat"],"metadata":{"id":"yhPNgPQh4XW_","executionInfo":{"status":"ok","timestamp":1676966359310,"user_tz":-540,"elapsed":319,"user":{"displayName":"오윤주","userId":"02786696648684205101"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["class TrainerDeepSVDD:\n","    def __init__(self, args, data_loader, device):\n","        self.args = args\n","        self.train_loader = data_loader\n","        self.device = device\n","\n","    def pretrain(self):\n","        \"\"\" DeepSVDD 모델에서 사용할 가중치를 학습시키는 AutoEncoder 학습 단계\"\"\"\n","        ae = pretrain_autoencoder(self.args.latent_dim).to(self.device)\n","        ae.apply(weights_init_normal)\n","        optimizer = torch.optim.Adam(ae.parameters(), lr=self.args.lr_ae,\n","                               weight_decay=self.args.weight_decay_ae)\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n","                    milestones=self.args.lr_milestones, gamma=0.1)\n","        \n","        ae.train()\n","        for epoch in range(self.args.num_epochs_ae):\n","            total_loss = 0\n","            for x, _ in self.train_loader:\n","                x = x.float().to(self.device)\n","                \n","                optimizer.zero_grad()\n","                x_hat = ae(x)\n","                reconst_loss = torch.mean(torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim()))))\n","                reconst_loss.backward()\n","                optimizer.step()\n","                \n","                total_loss += reconst_loss.item()\n","            scheduler.step()\n","            print('Pretraining Autoencoder... Epoch: {}, Loss: {:.3f}'.format(\n","                   epoch, total_loss/len(self.train_loader)))\n","        self.save_weights_for_DeepSVDD(ae, self.train_loader) \n","    \n","\n","    def save_weights_for_DeepSVDD(self, model, dataloader):\n","        \"\"\"학습된 AutoEncoder 가중치를 DeepSVDD모델에 Initialize해주는 함수\"\"\"\n","        c = self.set_c(model, dataloader)\n","        net = DeepSVDD_network(self.args.latent_dim).to(self.device)\n","        state_dict = model.state_dict()\n","        net.load_state_dict(state_dict, strict=False)\n","        torch.save({'center': c.cpu().data.numpy().tolist(),\n","                    'net_dict': net.state_dict()}, '/content/drive/MyDrive/weights/pretrained_parameters.pth')\n","    \n","\n","    def set_c(self, model, dataloader, eps=0.1):\n","        \"\"\"Initializing the center for the hypersphere\"\"\"\n","        model.eval()\n","        z_ = []\n","        with torch.no_grad():\n","            for x, _ in dataloader:\n","                x = x.float().to(self.device)\n","                z = model.encoder(x)\n","                z_.append(z.detach())\n","        z_ = torch.cat(z_)\n","        c = torch.mean(z_, dim=0)\n","        c[(abs(c) < eps) & (c < 0)] = -eps\n","        c[(abs(c) < eps) & (c > 0)] = eps\n","        return c\n","\n","    def train(self):\n","        \"\"\"Deep SVDD model 학습\"\"\"\n","        net = DeepSVDD_network().to(self.device)\n","        \n","        if self.args.pretrain==True:\n","            state_dict = torch.load('/content/drive/MyDrive/weights/pretrained_parameters.pth')\n","            net.load_state_dict(state_dict['net_dict'])\n","            c = torch.Tensor(state_dict['center']).to(self.device)\n","        else:\n","            net.apply(weights_init_normal)\n","            c = torch.randn(self.args.latent_dim).to(self.device)\n","        \n","        optimizer = torch.optim.Adam(net.parameters(), lr=self.args.lr,\n","                               weight_decay=self.args.weight_decay)\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n","                    milestones=self.args.lr_milestones, gamma=0.1)\n","\n","        net.train()\n","        for epoch in range(self.args.num_epochs):\n","            total_loss = 0\n","            for x, _ in self.train_loader:\n","                x = x.float().to(self.device)\n","\n","                optimizer.zero_grad()\n","                z = net(x)\n","                loss = torch.mean(torch.sum((z - c) ** 2, dim=1))\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","            scheduler.step()\n","            print('Training Deep SVDD... Epoch: {}, Loss: {:.3f}'.format(\n","                   epoch, total_loss/len(self.train_loader)))\n","        self.net = net\n","        self.c = c\n","\n","        return self.net, self.c\n","        \n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"Linear\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    \n","    def pretrain(self):\n","        \"\"\" DeepSVDD 모델에서 사용할 가중치를 학습시키는 AutoEncoder 학습 단계\"\"\"\n","        ae = pretrain_autoencoder(self.args.latent_dim).to(self.device)\n","        ae.apply(weights_init_normal)\n","        optimizer = torch.optim.Adam(ae.parameters(), lr=self.args.lr_ae,\n","                               weight_decay=self.args.weight_decay_ae)\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n","                    milestones=self.args.lr_milestones, gamma=0.1)\n","        \n","        ae.train()\n","        for epoch in range(self.args.num_epochs_ae):\n","            total_loss = 0\n","            for x, _ in self.train_loader:\n","                x = x.float().to(self.device)\n","                \n","                optimizer.zero_grad()\n","                x_hat = ae(x)\n","                reconst_loss = torch.mean(torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim()))))\n","                reconst_loss.backward()\n","                optimizer.step()\n","                \n","                total_loss += reconst_loss.item()\n","            scheduler.step()\n","            print('Pretraining Autoencoder... Epoch: {}, Loss: {:.3f}'.format(\n","                   epoch, total_loss/len(self.train_loader)))\n","        self.save_weights_for_DeepSVDD(ae, self.train_loader) \n","\n","        def save_weights_for_DeepSVDD(self, model, dataloader):\n","              \"\"\"학습된 AutoEncoder 가중치를 DeepSVDD모델에 Initialize해주는 함수\"\"\"\n","              c = self.set_c(model, dataloader)\n","              net = DeepSVDD_network(self.args.latent_dim).to(self.device)\n","              state_dict = model.state_dict()\n","              net.load_state_dict(state_dict, strict=False)\n","              torch.save({'center': c.cpu().data.numpy().tolist(),\n","                          'net_dict': net.state_dict()}, '/content/drive/MyDrive/weights/pretrained_parameters.pth')\n","\n","        def train(self):\n","            \"\"\"Deep SVDD model 학습\"\"\"    \n","            net = DeepSVDD_network().to(self.device)\n","            \n","            if self.args.pretrain==True:\n","                state_dict = torch.load('/content/drive/MyDrive/weights/pretrained_parameters.pth')\n","                net.load_state_dict(state_dict['net_dict'])\n","                c = torch.Tensor(state_dict['center']).to(self.device)\n","            else:\n","                net.apply(weights_init_normal)\n","                c = torch.randn(self.args.latent_dim).to(self.device)\n","            \n","            optimizer = torch.optim.Adam(net.parameters(), lr=self.args.lr,\n","                                  weight_decay=self.args.weight_decay)\n","            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n","                        milestones=self.args.lr_milestones, gamma=0.1)\n","\n","            net.train()\n","            for epoch in range(self.args.num_epochs):\n","                total_loss = 0\n","                for x, _ in self.train_loader:\n","                    x = x.float().to(self.device)\n","\n","                    optimizer.zero_grad()\n","                    z = net(x)\n","                    loss = torch.mean(torch.sum((z - c) ** 2, dim=1))\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                    total_loss += loss.item()\n","                scheduler.step()\n","                print('Training Deep SVDD... Epoch: {}, Loss: {:.3f}'.format(\n","                      epoch, total_loss/len(self.train_loader)))\n","            self.net = net\n","            self.c = c\n","\n","            return self.net, self.c"],"metadata":{"id":"9H_AU5KV5NlY","executionInfo":{"status":"ok","timestamp":1676966410413,"user_tz":-540,"elapsed":305,"user":{"displayName":"오윤주","userId":"02786696648684205101"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","args = easydict.EasyDict({\n","       'num_epochs':50,\n","       'num_epochs_ae':50,\n","       'lr':1e-3,\n","       'lr_ae':1e-3,\n","       'weight_decay':5e-7,\n","       'weight_decay_ae':5e-3,\n","       'lr_milestones':[50],\n","       'batch_size':1024,\n","       'pretrain':True,\n","       'latent_dim':32,\n","       'normal_class':0\n","                })\n","\n","if __name__ == '__main__':\n","\n","    # Train/Test Loader 불러오기\n","    dataloader_train, dataloader_test = get_mnist(args)\n","\n","    # Network 학습준비, 구조 불러오기\n","    deep_SVDD = TrainerDeepSVDD(args, dataloader_train, device)\n","\n","    # DeepSVDD를 위한 DeepLearning pretrain 모델로 Weight 학습\n","    if args.pretrain:\n","        deep_SVDD.pretrain()\n","\n","    # 학습된 가중치로 Deep_SVDD모델 Train\n","    net, c = deep_SVDD.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpO-T8IS5g-i","executionInfo":{"status":"ok","timestamp":1676967003023,"user_tz":-540,"elapsed":588801,"user":{"displayName":"오윤주","userId":"02786696648684205101"}},"outputId":"99f0de60-d9f4-44f1-8971-c6e77b2ba30a"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretraining Autoencoder... Epoch: 0, Loss: 115.170\n","Pretraining Autoencoder... Epoch: 1, Loss: 62.201\n","Pretraining Autoencoder... Epoch: 2, Loss: 32.786\n","Pretraining Autoencoder... Epoch: 3, Loss: 19.134\n","Pretraining Autoencoder... Epoch: 4, Loss: 13.375\n","Pretraining Autoencoder... Epoch: 5, Loss: 10.739\n","Pretraining Autoencoder... Epoch: 6, Loss: 9.342\n","Pretraining Autoencoder... Epoch: 7, Loss: 8.471\n","Pretraining Autoencoder... Epoch: 8, Loss: 7.882\n","Pretraining Autoencoder... Epoch: 9, Loss: 7.459\n","Pretraining Autoencoder... Epoch: 10, Loss: 7.121\n","Pretraining Autoencoder... Epoch: 11, Loss: 6.840\n","Pretraining Autoencoder... Epoch: 12, Loss: 6.585\n","Pretraining Autoencoder... Epoch: 13, Loss: 6.342\n","Pretraining Autoencoder... Epoch: 14, Loss: 6.141\n","Pretraining Autoencoder... Epoch: 15, Loss: 5.940\n","Pretraining Autoencoder... Epoch: 16, Loss: 5.753\n","Pretraining Autoencoder... Epoch: 17, Loss: 5.567\n","Pretraining Autoencoder... Epoch: 18, Loss: 5.386\n","Pretraining Autoencoder... Epoch: 19, Loss: 5.212\n","Pretraining Autoencoder... Epoch: 20, Loss: 5.047\n","Pretraining Autoencoder... Epoch: 21, Loss: 4.892\n","Pretraining Autoencoder... Epoch: 22, Loss: 4.746\n","Pretraining Autoencoder... Epoch: 23, Loss: 4.614\n","Pretraining Autoencoder... Epoch: 24, Loss: 4.482\n","Pretraining Autoencoder... Epoch: 25, Loss: 4.365\n","Pretraining Autoencoder... Epoch: 26, Loss: 4.264\n","Pretraining Autoencoder... Epoch: 27, Loss: 4.182\n","Pretraining Autoencoder... Epoch: 28, Loss: 4.087\n","Pretraining Autoencoder... Epoch: 29, Loss: 4.005\n","Pretraining Autoencoder... Epoch: 30, Loss: 3.912\n","Pretraining Autoencoder... Epoch: 31, Loss: 3.833\n","Pretraining Autoencoder... Epoch: 32, Loss: 3.762\n","Pretraining Autoencoder... Epoch: 33, Loss: 3.691\n","Pretraining Autoencoder... Epoch: 34, Loss: 3.629\n","Pretraining Autoencoder... Epoch: 35, Loss: 3.577\n","Pretraining Autoencoder... Epoch: 36, Loss: 3.516\n","Pretraining Autoencoder... Epoch: 37, Loss: 3.464\n","Pretraining Autoencoder... Epoch: 38, Loss: 3.411\n","Pretraining Autoencoder... Epoch: 39, Loss: 3.369\n","Pretraining Autoencoder... Epoch: 40, Loss: 3.327\n","Pretraining Autoencoder... Epoch: 41, Loss: 3.297\n","Pretraining Autoencoder... Epoch: 42, Loss: 3.275\n","Pretraining Autoencoder... Epoch: 43, Loss: 3.224\n","Pretraining Autoencoder... Epoch: 44, Loss: 3.190\n","Pretraining Autoencoder... Epoch: 45, Loss: 3.161\n","Pretraining Autoencoder... Epoch: 46, Loss: 3.119\n","Pretraining Autoencoder... Epoch: 47, Loss: 3.095\n","Pretraining Autoencoder... Epoch: 48, Loss: 3.064\n","Pretraining Autoencoder... Epoch: 49, Loss: 3.051\n","Training Deep SVDD... Epoch: 0, Loss: 0.941\n","Training Deep SVDD... Epoch: 1, Loss: 0.246\n","Training Deep SVDD... Epoch: 2, Loss: 0.132\n","Training Deep SVDD... Epoch: 3, Loss: 0.091\n","Training Deep SVDD... Epoch: 4, Loss: 0.063\n","Training Deep SVDD... Epoch: 5, Loss: 0.049\n","Training Deep SVDD... Epoch: 6, Loss: 0.040\n","Training Deep SVDD... Epoch: 7, Loss: 0.033\n","Training Deep SVDD... Epoch: 8, Loss: 0.027\n","Training Deep SVDD... Epoch: 9, Loss: 0.023\n","Training Deep SVDD... Epoch: 10, Loss: 0.021\n","Training Deep SVDD... Epoch: 11, Loss: 0.019\n","Training Deep SVDD... Epoch: 12, Loss: 0.017\n","Training Deep SVDD... Epoch: 13, Loss: 0.016\n","Training Deep SVDD... Epoch: 14, Loss: 0.019\n","Training Deep SVDD... Epoch: 15, Loss: 0.016\n","Training Deep SVDD... Epoch: 16, Loss: 0.016\n","Training Deep SVDD... Epoch: 17, Loss: 0.014\n","Training Deep SVDD... Epoch: 18, Loss: 0.012\n","Training Deep SVDD... Epoch: 19, Loss: 0.013\n","Training Deep SVDD... Epoch: 20, Loss: 0.013\n","Training Deep SVDD... Epoch: 21, Loss: 0.011\n","Training Deep SVDD... Epoch: 22, Loss: 0.010\n","Training Deep SVDD... Epoch: 23, Loss: 0.009\n","Training Deep SVDD... Epoch: 24, Loss: 0.010\n","Training Deep SVDD... Epoch: 25, Loss: 0.011\n","Training Deep SVDD... Epoch: 26, Loss: 0.011\n","Training Deep SVDD... Epoch: 27, Loss: 0.010\n","Training Deep SVDD... Epoch: 28, Loss: 0.009\n","Training Deep SVDD... Epoch: 29, Loss: 0.008\n","Training Deep SVDD... Epoch: 30, Loss: 0.007\n","Training Deep SVDD... Epoch: 31, Loss: 0.008\n","Training Deep SVDD... Epoch: 32, Loss: 0.007\n","Training Deep SVDD... Epoch: 33, Loss: 0.008\n","Training Deep SVDD... Epoch: 34, Loss: 0.007\n","Training Deep SVDD... Epoch: 35, Loss: 0.007\n","Training Deep SVDD... Epoch: 36, Loss: 0.007\n","Training Deep SVDD... Epoch: 37, Loss: 0.007\n","Training Deep SVDD... Epoch: 38, Loss: 0.006\n","Training Deep SVDD... Epoch: 39, Loss: 0.006\n","Training Deep SVDD... Epoch: 40, Loss: 0.007\n","Training Deep SVDD... Epoch: 41, Loss: 0.006\n","Training Deep SVDD... Epoch: 42, Loss: 0.005\n","Training Deep SVDD... Epoch: 43, Loss: 0.008\n","Training Deep SVDD... Epoch: 44, Loss: 0.009\n","Training Deep SVDD... Epoch: 45, Loss: 0.007\n","Training Deep SVDD... Epoch: 46, Loss: 0.006\n","Training Deep SVDD... Epoch: 47, Loss: 0.007\n","Training Deep SVDD... Epoch: 48, Loss: 0.010\n","Training Deep SVDD... Epoch: 49, Loss: 0.008\n"]}]},{"cell_type":"code","source":["def eval(net, c, dataloader, device):\n","    \"\"\"Testing the Deep SVDD model\"\"\"\n","\n","    scores = []\n","    labels = []\n","    net.eval()\n","    print('Testing...')\n","    with torch.no_grad():\n","        for x, y in dataloader:\n","            x = x.float().to(device)\n","            z = net(x)\n","            score = torch.sum((z - c) ** 2, dim=1)\n","\n","            scores.append(score.detach().cpu())\n","            labels.append(y.cpu())\n","    labels, scores = torch.cat(labels).numpy(), torch.cat(scores).numpy()\n","    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n","    return labels, scores"],"metadata":{"id":"OGxAvTzC5nms","executionInfo":{"status":"ok","timestamp":1676967129160,"user_tz":-540,"elapsed":1009,"user":{"displayName":"오윤주","userId":"02786696648684205101"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["print(eval(net, c, dataloader_test, device))\n","\n","## mnist 데이터 중 1만 정상 데이터로 판별, test 데이터는 mnist test data 전체 사용"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUeRsAaIBwR9","executionInfo":{"status":"ok","timestamp":1676967187910,"user_tz":-540,"elapsed":3954,"user":{"displayName":"오윤주","userId":"02786696648684205101"}},"outputId":"9b47563c-8720-4497-91b9-fcd585202605"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing...\n","ROC AUC score: 98.28\n","(array([1, 1, 1, ..., 1, 1, 1]), array([0.02881614, 0.11197484, 0.14798847, ..., 0.03123659, 0.01924382,\n","       0.03086499], dtype=float32))\n"]}]}]}